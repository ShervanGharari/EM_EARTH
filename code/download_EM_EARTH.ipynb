{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffe942c",
   "metadata": {},
   "source": [
    "# Code to download part of EM-EARTH data for a given variable(s), year(s), or region(s)\n",
    "\n",
    "\n",
    "## For using the code, linux system is needed and wget should be available.\n",
    "\n",
    "## save frdr-checksums-0547.csv from the link https://drive.google.com/file/d/1HWNNCBpalAymIouburwm7flDhcjqWcqN/view?usp=share_link to the folder hash with the name frdr-checksums-0547.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f982456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import hashlib\n",
    "import glob\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gdown\n",
    "\n",
    "def download_EM_ERATH (server = 'https://g-772fa5.cd4fe.0ec8.data.globus.org/6/published/publication_542/submitted_data/',\n",
    "                       groups = ['deterministic_hourly','deterministic_raw_daily',\\\n",
    "                                 'probabilistic_daily'],\n",
    "                       variables = ['prcp','tdew','tmean','trange'],\n",
    "                       regions = ['Asia','Europe','Africa',\\\n",
    "                                  'NorthAmerica','Oceania','SouthAmerica'],\n",
    "                       years = ['1950','1951','1952','1953','1954',\\\n",
    "                                '1955','1956','1957','1958','1959',\\\n",
    "                                '1960','1961','1962','1963','1964',\\\n",
    "                                '1965','1966','1967','1968','1969',\\\n",
    "                                '1970','1971','1972','1973','1974',\\\n",
    "                                '1975','1976','1977','1978','1979',\\\n",
    "                                '1980','1981','1982','1983','1984',\\\n",
    "                                '1985','1986','1987','1988','1989',\\\n",
    "                                '1990','1991','1992','1993','1994',\\\n",
    "                                '1995','1996','1997','1998','1999',\\\n",
    "                                '2000','2001','2002','2003','2004',\\\n",
    "                                '2005','2006','2007','2008','2009',\\\n",
    "                                '2010','2011','2012','2013','2014',\\\n",
    "                                '2015','2016','2017','2018','2019'],\n",
    "                       months = ['01','02','03','04','05','06',\\\n",
    "                                 '07','08','09','10','11','12'],\n",
    "                       ensembles = ['001','002','003','004','005',\\\n",
    "                                    '006','007','008','009','010',\\\n",
    "                                    '011','012','013','014','015',\\\n",
    "                                    '016','017','018','019','020',\\\n",
    "                                    '021','022','023','024','025'],\n",
    "                       root_save = '../EM_Earth_v1/'):\n",
    "    \n",
    "    \n",
    "    # get the csv\n",
    "    url = \"https://drive.google.com/uc?id=1HWNNCBpalAymIouburwm7flDhcjqWcqN\"\n",
    "    output = \"../hash/frdr-checksums-0547.csv\"\n",
    "    gdown.download(url, output, quiet=False)\n",
    "    df = pd.read_csv(output)\n",
    "    \n",
    "    # \n",
    "    for group in groups:\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                for variable in variables:\n",
    "                    \n",
    "                    # if the case is deterministic raw daily\n",
    "                    if group == 'deterministic_raw_daily':\n",
    "                        #\n",
    "                        terms = [group, year+month, variable] # terms are the terms \n",
    "                        df_slice = df.copy()\n",
    "                        for term in terms: # loop over terms to get the target file names\n",
    "                            #\n",
    "                            df_slice = df_slice[df_slice['File_Name'].str.contains(term)]\n",
    "                        \n",
    "                        for index, row in df_slice.iterrows():\n",
    "                            \n",
    "                            file_name, path_name, link_name = \\\n",
    "                            prepare_file_name_path_name(row['File_Name'],root_save,server)\n",
    "                            \n",
    "                            downlaod (file_name, path_name, link_name, row['sha256'].strip())\n",
    "                            \n",
    "                    \n",
    "                    \n",
    "                    for region in regions:\n",
    "                        if group == 'deterministic_hourly' and variable != 'trange':\n",
    "                            #\n",
    "                            terms = [group, year+month, variable, region] # terms are the terms \n",
    "                            df_slice = df.copy()\n",
    "                            \n",
    "                            for term in terms: # loop over terms to get the target file names\n",
    "                                #\n",
    "                                df_slice = df_slice[df_slice['File_Name'].str.contains(term)]\n",
    "                        \n",
    "                            for index, row in df_slice.iterrows():\n",
    "\n",
    "                                file_name, path_name, link_name = \\\n",
    "                                prepare_file_name_path_name(row['File_Name'],root_save,server)\n",
    "\n",
    "                                downlaod (file_name, path_name, link_name, row['sha256'].strip())\n",
    "                                \n",
    "                        if group == 'probabilistic_daily':\n",
    "\n",
    "                            for ensemble in ensembles:\n",
    "\n",
    "                                #\n",
    "                                terms = [group, year+month, variable, region, '_'+ensemble] # terms are the terms \n",
    "                                df_slice = df.copy()\n",
    "                                \n",
    "                                for term in terms: # loop over terms to get the target file names\n",
    "                                    #\n",
    "                                    df_slice = df_slice[df_slice['File_Name'].str.contains(term)]\n",
    "                        \n",
    "                                for index, row in df_slice.iterrows():\n",
    "\n",
    "                                    file_name, path_name, link_name = \\\n",
    "                                    prepare_file_name_path_name(row['File_Name'],root_save,server)\n",
    "\n",
    "                                    downlaod (file_name, path_name, link_name, row['sha256'].strip())\n",
    "\n",
    "def get_hash (file_name):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_name,\"rb\") as f:\n",
    "        # Read and update hash string value in blocks of 4K\n",
    "        for byte_block in iter(lambda: f.read(4096),b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest().strip()\n",
    "    \n",
    "def prepare_file_name_path_name(full_name,\n",
    "                                root_save,\n",
    "                                server):\n",
    "    file_path = re.sub('/globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/',\\\n",
    "                       '',\\\n",
    "                       full_name)\n",
    "    file_name = file_path.split(\"/\")[-1].strip()\n",
    "    path_name = re.sub(file_name,'',file_path).strip()\n",
    "    link_name = server+'EM_Earth_v1/'+path_name+file_name\n",
    "    path_name = root_save+path_name\n",
    "    \n",
    "    return file_name, path_name, link_name\n",
    "\n",
    "def downlaod (file_name, path_name, link_name, hash_value_remote):\n",
    "    \n",
    "    #\n",
    "    downloaded = False\n",
    "    try_number = 1\n",
    "            \n",
    "    \n",
    "    # directory\n",
    "    if not os.path.isdir(path_name):\n",
    "        os.makedirs(path_name)\n",
    "    \n",
    "    # check if the file exists on local directory\n",
    "    if os.path.isfile(path_name+file_name):\n",
    "        \n",
    "        # check the hash value\n",
    "        hash_value_local = get_hash (path_name+file_name)\n",
    "        \n",
    "        if hash_value_local == hash_value_remote:\n",
    "            \n",
    "            #\n",
    "            downloaded = True\n",
    "            \n",
    "    while (not downloaded) and (try_number < 1000):\n",
    "\n",
    "#         r = requests.get(link_name) # download the URL\n",
    "#         # print the specification of the download \n",
    "#         print(r.status_code, r.headers['content-type'], r.encoding)\n",
    "#         # if download successful the statuse code is 200 then save the file, else print what was not downloaded\n",
    "#         if r.status_code == 200:\n",
    "#             print('download was successful for '+link_name)\n",
    "#             with open(path_name+file_name, 'wb') as f:\n",
    "#                 f.write(r.content)\n",
    "#         else:\n",
    "#             print('download was not successful for '+link_name)\n",
    "\n",
    "        os.system('wget '+link_name+' -O '+path_name+file_name)\n",
    "\n",
    "        hash_value_local = get_hash (path_name+file_name)\n",
    "        \n",
    "        print(hash_value_local)\n",
    "        print(hash_value_remote)\n",
    "\n",
    "        if hash_value_local == hash_value_remote:\n",
    "            downloaded = True\n",
    "\n",
    "        try_number = try_number + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5dfb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of download\n",
    "download_EM_ERATH (variables = ['prcp','tdew','tmean','trange'],\n",
    "                   years = ['1950','1951'],\n",
    "                   months = ['01','02','03'],\n",
    "                   ensembles = ['001','025'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "myenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
