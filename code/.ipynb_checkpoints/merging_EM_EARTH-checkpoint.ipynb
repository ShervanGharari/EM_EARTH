{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e27567",
   "metadata": {},
   "source": [
    "## Merging the regional nc file to global ones for ensemble members and also deterministic hourly values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b883ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import hashlib\n",
    "import glob\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merging_EM_ERATH_ensmebles  (variables = ['prcp','tdew','tmean','trange'],\n",
    "                                 regions = ['Asia','Europe','Africa',\\\n",
    "                                            'NorthAmerica','Oceania','SouthAmerica'],\n",
    "                                 years = ['1950','1951','1952','1953','1954',\\\n",
    "                                          '1955','1956','1957','1958','1959',\\\n",
    "                                          '1960','1961','1962','1963','1964',\\\n",
    "                                          '1965','1966','1967','1968','1969',\\\n",
    "                                          '1970','1971','1972','1973','1974',\\\n",
    "                                          '1975','1976','1977','1978','1979',\\\n",
    "                                          '1980','1981','1982','1983','1984',\\\n",
    "                                          '1985','1986','1987','1988','1989',\\\n",
    "                                          '1990','1991','1992','1993','1994',\\\n",
    "                                          '1995','1996','1997','1998','1999',\\\n",
    "                                          '2000','2001','2002','2003','2004',\\\n",
    "                                          '2005','2006','2007','2008','2009',\\\n",
    "                                          '2010','2011','2012','2013','2014',\\\n",
    "                                          '2015','2016','2017','2018','2019'],\n",
    "                                 months = ['01','02','03','04','05','06',\\\n",
    "                                           '07','08','09','10','11','12'],\n",
    "                                 ensembles = ['001','002','003','004','005',\\\n",
    "                                              '006','007','008','009','010',\\\n",
    "                                              '011','012','013','014','015',\\\n",
    "                                              '016','017','018','019','020',\\\n",
    "                                              '021','022','023','024','025'],\n",
    "                                 root_save = '../EM_Earth_v1/'):\n",
    "    \n",
    "    \n",
    "    for year in years:\n",
    "\n",
    "        for month in months:\n",
    "\n",
    "            for variable in variables:\n",
    "                \n",
    "                # file_name\n",
    "                file_name = root_save+'deterministic_raw_daily/'+variable+'/'+\\\n",
    "                'EM_Earth_deterministic_daily_'+variable+'_'+year+month+'.nc'\n",
    "                \n",
    "                #\n",
    "                if os.path.isfile(file_name):\n",
    "                \n",
    "                    # open dataset\n",
    "                    ds_global = xr.open_dataset(file_name)\n",
    "                    #print(ds_global.lon[0:3])\n",
    "\n",
    "                    # reorganize the variables to time, lat, lon\n",
    "                    ds_global = ds_global.transpose('time', 'lat', 'lon')\n",
    "\n",
    "                    mask_sum ={}\n",
    "                    # replace the varibales with 0.00\n",
    "                    for v in ds_global.data_vars:\n",
    "                        ds_global [v][:] = 0.00\n",
    "                        mask_sum[v] = np.zeros([1800, 3600])\n",
    "\n",
    "                    # loop over the ensembles\n",
    "                    for ensemble in ensembles:\n",
    "\n",
    "                        #  read the files\n",
    "                        file_names = root_save+'probabilistic_daily/'+variable+'/*/'+\\\n",
    "                                               year+month+'/*'+ensemble+'.nc'\n",
    "                        file_names = glob.glob(file_names)\n",
    "\n",
    "                        if file_names:\n",
    "\n",
    "                            #\n",
    "                            for file_name in file_names:\n",
    "                                # print(file_name)\n",
    "                                ds = xr.open_dataset(file_name)\n",
    "                                # print(ds.data_vars)\n",
    "                                ds = ds.transpose('time', 'lat', 'lon') # making sure the order\n",
    "\n",
    "                                if ds.lon.values.max() > 180:\n",
    "                                    # Asia\n",
    "                                    ds1 = ds.sel(lon=slice(None, 179.975))\n",
    "                                    ds2 = ds.sel(lon=slice(179.975, None))\n",
    "                                    ds2['lon'] = ds2['lon'] - 360\n",
    "                                    ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                                    ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                                elif ds.lon.values.min() < -180:\n",
    "                                    # North America\n",
    "                                    ds1 = ds.sel(lon=slice(None, -179.975))\n",
    "                                    ds2 = ds.sel(lon=slice(-179.975, None))\n",
    "                                    ds1['lon'] = ds1['lon'] + 360\n",
    "                                    ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                                    ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                                else:\n",
    "                                    ds_global, mask_sum = fill_cont_to_globe(ds , ds_global, mask_sum)\n",
    "\n",
    "\n",
    "                            file_name_save = 'EM_Earth_probabilistic_daily_'+variable+'_global_'+\\\n",
    "                                        year+month+'_'+ensemble+'.nc'\n",
    "                            folder_name = root_save+'probabilistic_daily_global/'+variable+'/'+\\\n",
    "                                        year+month+'/'\n",
    "                            if not os.path.isdir(folder_name):\n",
    "                                os.makedirs(folder_name)\n",
    "\n",
    "                            for name, value in ds_global.items():\n",
    "                                for t in range(value.shape[0]):\n",
    "                                    temp = np.array(value[t,:,:]) / mask_sum[name]\n",
    "                                    temp[mask_sum[name] == 0] = -9999\n",
    "                                    value[t, :, :] = temp\n",
    "                                ds_global[name] = value\n",
    "\n",
    "                            # print(file_name_save)\n",
    "\n",
    "                            os.system('rm '+folder_name+file_name_save)\n",
    "\n",
    "                            encoding = {}\n",
    "                            for var in ds_global.data_vars:\n",
    "                                encoding[var] = {'_FillValue': -9999., 'zlib': True, 'complevel': 4}\n",
    "\n",
    "                            ds_global.to_netcdf(folder_name+file_name_save, unlimited_dims='time', encoding=encoding)\n",
    "                            \n",
    "                            #\n",
    "                            test_merging (file_names, folder_name+file_name_save,  year, month, variable)\n",
    "                            \n",
    "                            # close\n",
    "                            ds.close()\n",
    "                            ds1.close()\n",
    "                            ds2.close()\n",
    "                            ds_global.close()\n",
    "                            \n",
    "\n",
    "def merging_EM_ERATH_deterministic  (variables = ['prcp','tdew','tmean'],\n",
    "                                     regions = ['Asia','Europe','Africa',\\\n",
    "                                                'NorthAmerica','Oceania','SouthAmerica'],\n",
    "                                     years = ['1950','1951','1952','1953','1954',\\\n",
    "                                              '1955','1956','1957','1958','1959',\\\n",
    "                                              '1960','1961','1962','1963','1964',\\\n",
    "                                              '1965','1966','1967','1968','1969',\\\n",
    "                                              '1970','1971','1972','1973','1974',\\\n",
    "                                              '1975','1976','1977','1978','1979',\\\n",
    "                                              '1980','1981','1982','1983','1984',\\\n",
    "                                              '1985','1986','1987','1988','1989',\\\n",
    "                                              '1990','1991','1992','1993','1994',\\\n",
    "                                              '1995','1996','1997','1998','1999',\\\n",
    "                                              '2000','2001','2002','2003','2004',\\\n",
    "                                              '2005','2006','2007','2008','2009',\\\n",
    "                                              '2010','2011','2012','2013','2014',\\\n",
    "                                              '2015','2016','2017','2018','2019'],\n",
    "                                     months = ['01','02','03','04','05','06',\\\n",
    "                                               '07','08','09','10','11','12'],\n",
    "                                     root_save = '../EM_Earth_v1/'):\n",
    "    \n",
    "    \n",
    "    for year in years:\n",
    "\n",
    "        for month in months:\n",
    "\n",
    "            for variable in variables:\n",
    "                \n",
    "                # file_name\n",
    "                file_name_global = root_save+'deterministic_raw_daily/'+variable+'/'+\\\n",
    "                'EM_Earth_deterministic_daily_'+variable+'_'+year+month+'.nc'\n",
    "                file_name_times = glob.glob(root_save+'deterministic_hourly/'+variable+'/*/'+\\\n",
    "                'EM_Earth_deterministic_hourly_*_'+year+month+'.nc')\n",
    "                \n",
    "                #\n",
    "                if os.path.isfile(file_name_global) and file_name_times:\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    # open dataset\n",
    "                    ds_global = xr.open_dataset(file_name_global)\n",
    "                    ds_time = xr.open_dataset(file_name_times[0])\n",
    "\n",
    "                    # go from daily to hourly values\n",
    "                    ds_temp = ds_global.copy()\n",
    "                    temp = ds_temp['time'].copy()\n",
    "                    ds_temp = ds_temp.drop(labels='time')\n",
    "                    ds_temp['time'] = temp + np.timedelta64(23, 'h')\n",
    "                    ds_temp = ds_temp.isel(time=[-2,-1])\n",
    "                    ds_global = xr.merge([ds_global, ds_temp])\n",
    "                    ds_global = ds_global.resample(time='1H').pad()\n",
    "                    ds_global = ds_global.drop(labels='time')\n",
    "                    ds_global['time'] = ds_time['time']\n",
    "                    \n",
    "                    # reorganize the variables to time, lat, lon\n",
    "                    ds_global = ds_global.transpose('time', 'lat', 'lon')\n",
    "\n",
    "                    mask_sum ={}\n",
    "                    # replace the varibales with 0.00\n",
    "                    for v in ds_global.data_vars:\n",
    "                        ds_global [v][:] = 0.00\n",
    "                        mask_sum[v] = np.zeros([1800, 3600])\n",
    "                        \n",
    "                    #  read the files\n",
    "                    file_names = root_save+'deterministic_hourly/'+variable+'/*/*'+\\\n",
    "                                           year+month+'*.nc'\n",
    "                    file_names = glob.glob(file_names)\n",
    "                    \n",
    "                    file_names = file_names[0:2]\n",
    "                    \n",
    "                    \n",
    "                    if file_names:\n",
    "                        \n",
    "                        file_names = file_names\n",
    "\n",
    "                        #\n",
    "                        for file_name in file_names:\n",
    "                            \n",
    "                            ds = xr.open_dataset(file_name)\n",
    "                            \n",
    "                            ds = ds.transpose('time', 'lat', 'lon') # making sure the order\n",
    "\n",
    "                            if ds.lon.values.max() > 180:\n",
    "                                # Asia\n",
    "                                ds1 = ds.sel(lon=slice(None, 179.975))\n",
    "                                ds2 = ds.sel(lon=slice(179.975, None))\n",
    "                                ds2['lon'] = ds2['lon'] - 360\n",
    "                                ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                                ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                            elif ds.lon.values.min() < -180:\n",
    "                                # North America\n",
    "                                ds1 = ds.sel(lon=slice(None, -179.975))\n",
    "                                ds2 = ds.sel(lon=slice(-179.975, None))\n",
    "                                ds1['lon'] = ds1['lon'] + 360\n",
    "                                ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                                ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                            else:\n",
    "                                ds_global, mask_sum = fill_cont_to_globe(ds , ds_global, mask_sum)\n",
    "\n",
    "\n",
    "                        file_name_save = 'EM_Earth_deterministic_hourly_'+variable+'_global_'+\\\n",
    "                                    year+month+'.nc'\n",
    "                        folder_name = root_save+'deterministic_hourly_global/'+variable+'/'+\\\n",
    "                                    year+month+'/'\n",
    "                        \n",
    "                        if not os.path.isdir(folder_name):\n",
    "                            os.makedirs(folder_name)\n",
    "                            \n",
    "                        \n",
    "                        for name, value in ds_global.items():\n",
    "                            for t in range(value.shape[0]):\n",
    "                                temp = np.array(value[t,:,:]) / mask_sum[name]\n",
    "                                temp[mask_sum[name] == 0] = -9999\n",
    "                                value[t, :, :] = temp\n",
    "                            ds_global[name] = value\n",
    "\n",
    "                        os.system('rm '+folder_name+file_name_save)\n",
    "\n",
    "                        encoding = {}\n",
    "                        for var in ds_global.data_vars:\n",
    "                            encoding[var] = {'_FillValue': -9999., 'zlib': True, 'complevel': 4}\n",
    "\n",
    "                        ds_global.to_netcdf(folder_name+file_name_save, unlimited_dims='time', encoding=encoding)\n",
    "                        \n",
    "                        #\n",
    "                        test_merging (file_names, folder_name+file_name_save, year, month, variable)\n",
    "                        \n",
    "                        # close\n",
    "                        ds.close()\n",
    "                        ds1.close()\n",
    "                        ds2.close()\n",
    "                        ds_global.close()\n",
    "                        \n",
    "                        \n",
    "def fill_cont_to_globe(ds, ds_global, mask_sum):\n",
    "    \n",
    "    #\n",
    "    row_start = np.where(np.array(ds_global.lat[:]) == ds.lat.max().item())\n",
    "    row_end   = np.where(np.array(ds_global.lat[:]) == ds.lat.min().item())\n",
    "    col_start = np.where(np.array(ds_global.lon[:]) == ds.lon.min().item())\n",
    "    col_end   = np.where(np.array(ds_global.lon[:]) == ds.lon.max().item())\n",
    "    \n",
    "    #\n",
    "    row_start = row_start[0].item()\n",
    "    row_end   = row_end[0].item()\n",
    "    col_start = col_start[0].item()\n",
    "    col_end   = col_end[0].item()\n",
    "    \n",
    "    # print(row_start, row_end, col_start, col_end)\n",
    "    \n",
    "    for v in ds.data_vars:\n",
    "        \n",
    "        #\n",
    "        datav = ds[v].values.copy()\n",
    "        datav[np.isnan(datav)] = 0 # replace NaN with 0\n",
    "        ds_global[v][:, row_start:row_end+1, col_start:col_end+1] = \\\n",
    "        ds_global[v][:, row_start:row_end+1, col_start:col_end+1] + datav\n",
    "\n",
    "        #\n",
    "        datav = ds[v].values.copy()\n",
    "        datav = np.array(datav[-1,:,:])\n",
    "\n",
    "        datav[~np.isnan(datav)] = 1\n",
    "        datav[np.isnan(datav)] = 0\n",
    "        mask_sum[v][row_start:row_end+1, col_start:col_end+1] = \\\n",
    "        mask_sum[v][row_start:row_end+1, col_start:col_end+1] + datav\n",
    "        \n",
    "    \n",
    "    return ds_global, mask_sum\n",
    "\n",
    "\n",
    "def test_merging (regional_file_names, global_file_name, year, month, variable, time_step = 25):\n",
    "    \n",
    "    #\n",
    "    for file_name in regional_file_names:\n",
    "\n",
    "        #\n",
    "        ds_global = xr.open_dataset(global_file_name)\n",
    "        ds_regional = xr.open_dataset(file_name)\n",
    "\n",
    "        # global merged data\n",
    "        ds_global = ds_global.isel(time=[time_step]) #'2000-01-01', method='nearest')\n",
    "        ds_global = ds_global.squeeze()\n",
    "        ds_global = ds_global[variable].to_dataframe()\n",
    "        ds_global = ds_global.reset_index()\n",
    "        ds_global = ds_global.drop(columns=['time'])\n",
    "        ds_global['lat'] = ds_global['lat'].round(2)\n",
    "        ds_global['lon'] = ds_global['lon'].round(2)\n",
    "\n",
    "        # regional data\n",
    "        ds_regional = ds_regional.isel(time=[time_step]) #'2000-01-01', method='nearest')\n",
    "        ds_regional = ds_regional.squeeze()\n",
    "        ds_regional = ds_regional[variable].to_dataframe()\n",
    "        ds_regional = ds_regional.reset_index()\n",
    "        ds_regional = ds_regional.drop(columns=['time'])\n",
    "        ds_regional['lat'] = ds_regional['lat'].round(2)\n",
    "        ds_regional['lon'] = ds_regional['lon'].round(2)\n",
    "\n",
    "        # compare\n",
    "        ds_new = pd.merge(ds_global, ds_regional,  how='right', left_on=['lat','lon'], right_on = ['lat','lon'])\n",
    "        ds_new['diff'] = ds_new[variable+'_x']-ds_new[variable+'_x']\n",
    "        ds_new = ds_new.dropna()\n",
    "\n",
    "        #\n",
    "        if ds_new['diff'].abs().sum() > 0.001:\n",
    "            print('issue with merging of files', file_name,' in ',global_file_name)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the daily ensembmle from regional to global\n",
    "merging_EM_ERATH_ensmebles ()\n",
    "# merging the deterministic values from regional to global\n",
    "merging_EM_ERATH_deterministic ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3f991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "myenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
