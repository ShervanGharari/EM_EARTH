{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fffbba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download the EM-EARTH data and combine it to the model simulation\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import hashlib\n",
    "import glob\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def download_EM_ERATH (server = 'https://g-772fa5.cd4fe.0ec8.data.globus.org/6/published/publication_542/submitted_data/',\n",
    "                       groups = ['deterministic_hourly','deterministic_raw_daily','probabilistic_daily'],\n",
    "                       variables = ['prcp','tdew','tmean','trange'],\n",
    "                       regions = ['Asia','Europe','Africa','NorthAmerica','Oceania','SouthAmerica'],\n",
    "                       years = ['1950','1951','1952','1953','1954',\\\n",
    "                                '1955','1956','1957','1958','1959',\\\n",
    "                                '1960','1961','1962','1963','1964',\\\n",
    "                                '1965','1966','1967','1968','1969',\\\n",
    "                                '1970','1971','1972','1973','1974',\\\n",
    "                                '1975','1976','1977','1978','1979',\\\n",
    "                                '1980','1981','1982','1983','1984',\\\n",
    "                                '1985','1986','1987','1988','1989',\\\n",
    "                                '1990','1991','1992','1993','1994',\\\n",
    "                                '1995','1996','1997','1998','1999',\\\n",
    "                                '2000','2001','2002','2003','2004',\\\n",
    "                                '2005','2006','2007','2008','2009',\\\n",
    "                                '2010','2011','2012','2013','2014',\\\n",
    "                                '2015','2016','2017','2018','2019'],\n",
    "                       months = ['01','02','03','04','05','06',\\\n",
    "                                 '07','08','09','10','11','12'],\n",
    "                       ensembles = ['001','002','003','004','005',\\\n",
    "                                    '006','007','008','009','010',\\\n",
    "                                    '011','012','013','014','015',\\\n",
    "                                    '016','017','018','019','020',\\\n",
    "                                    '021','022','023','024','025'],\n",
    "                       root_save = '../EM_Earth_v1/'):\n",
    "    \n",
    "    \n",
    "    for group in groups:\n",
    "        \n",
    "        for year in years:\n",
    "            \n",
    "            for month in months:\n",
    "                \n",
    "                for variable in variables:\n",
    "                    \n",
    "                    if group == 'deterministic_raw_daily':\n",
    "                        \n",
    "                        file_name = 'EM_Earth_deterministic_daily_'+variable+'_'+year+month+'.nc'\n",
    "                        path      = group+'/'+variable+'/'\n",
    "                        link      = server+'EM_Earth_v1/'+path+file_name\n",
    "                        path_save = root_save + path\n",
    "                        if not os.path.isdir(path_save):\n",
    "                            os.makedirs(path_save)\n",
    "                        download_file (link,path_save,file_name)\n",
    "                    \n",
    "                    for region in regions:\n",
    "                            \n",
    "                            if group == 'deterministic_hourly' and variable != 'trange':\n",
    "                                \n",
    "                                file_name = 'EM_Earth_'+group+'_'+region+'_'+year+month+'.nc'\n",
    "                                path      = group+'/'+variable+'/'+region+'/'\n",
    "                                link      = server+'EM_Earth_v1/'+path+file_name\n",
    "                                path_save = root_save + path\n",
    "                                if not os.path.isdir(path_save):\n",
    "                                    os.makedirs(path_save)\n",
    "                                download_file (link,path_save,file_name)\n",
    "                        \n",
    "                            \n",
    "                            if group == 'probabilistic_daily':\n",
    "\n",
    "                                for ensemble in ensembles:\n",
    "\n",
    "                                    file_name = 'EM_Earth_probabilistic_daily_'+variable+'_'+region+'_'+\\\n",
    "                                    year+month+'_'+ensemble+'.nc'\n",
    "                                    path      = group+'/'+variable+'/'+region+'/'+year+month+'/'\n",
    "                                    link      = server+'EM_Earth_v1/'+path+file_name\n",
    "                                    path_save = root_save + path\n",
    "                                    if not os.path.isdir(path_save):\n",
    "                                        os.makedirs(path_save)\n",
    "                                    download_file (link,path_save,file_name)\n",
    "\n",
    "\n",
    "def download_file (link,path_save,file_name):\n",
    "    \n",
    "    \n",
    "    os.system('rm *.nc*')\n",
    "    downloaded = False\n",
    "    try_number = 1\n",
    "    while (not downloaded) and (try_number < 1000):\n",
    "        print('going to download '+file_name + ': '+str(try_number))\n",
    "        os.system('wget -nv '+link +' '+file_name)\n",
    "        #os.system('wget -O - link | tee file_name | md5sum > remote')\n",
    "        os.system('wget -O - '+link +' | md5sum > remote')\n",
    "        os.system('md5sum '+file_name+' > local')\n",
    "        md5_local = word_file('local')\n",
    "        md5_remote = word_file('remote')\n",
    "        print(md5_local, md5_remote)\n",
    "        if md5_local == md5_remote:\n",
    "            print('files are the same')\n",
    "            downloaded = True\n",
    "            os.system('mv '+ file_name +' '+path_save+file_name) # maybe test with mv\n",
    "        try_number = try_number + 1\n",
    "        os.system('rm *.nc*')\n",
    "\n",
    "\n",
    "def word_file(f):\n",
    "    with open(f) as file:\n",
    "        return file.read().split()[0]\n",
    "\n",
    "def merging_EM_ERATH_ensmebles  (variables = ['prcp','tdew','tmean','trange'],\n",
    "                                 regions = ['Asia','Europe','Africa','NorthAmerica','Oceania','SouthAmerica'],\n",
    "                                 years = ['1950','1951','1952','1953','1954',\\\n",
    "                                          '1955','1956','1957','1958','1959',\\\n",
    "                                          '1960','1961','1962','1963','1964',\\\n",
    "                                          '1965','1966','1967','1968','1969',\\\n",
    "                                          '1970','1971','1972','1973','1974',\\\n",
    "                                          '1975','1976','1977','1978','1979',\\\n",
    "                                          '1980','1981','1982','1983','1984',\\\n",
    "                                          '1985','1986','1987','1988','1989',\\\n",
    "                                          '1990','1991','1992','1993','1994',\\\n",
    "                                          '1995','1996','1997','1998','1999',\\\n",
    "                                          '2000','2001','2002','2003','2004',\\\n",
    "                                          '2005','2006','2007','2008','2009',\\\n",
    "                                          '2010','2011','2012','2013','2014',\\\n",
    "                                          '2015','2016','2017','2018','2019'],\n",
    "                                 months = ['01','02','03','04','05','06',\\\n",
    "                                           '07','08','09','10','11','12'],\n",
    "                                 ensembles = ['001','002','003','004','005',\\\n",
    "                                              '006','007','008','009','010',\\\n",
    "                                              '011','012','013','014','015',\\\n",
    "                                              '016','017','018','019','020',\\\n",
    "                                              '021','022','023','024','025'],\n",
    "                                 root_save = '../EM_Earth_v1/'):\n",
    "    \n",
    "    \n",
    "    for year in years:\n",
    "\n",
    "        for month in months:\n",
    "\n",
    "            for variable in variables:\n",
    "                \n",
    "                # file_name\n",
    "                file_name = root_save+'deterministic_raw_daily/'+variable+'/'+\\\n",
    "                'EM_Earth_deterministic_daily_'+variable+'_'+year+month+'.nc'\n",
    "                \n",
    "                # open dataset\n",
    "                ds_global = xr.open_dataset(file_name)\n",
    "                #print(ds_global.lon[0:3])\n",
    "                \n",
    "                # reorganize the variables to time, lat, lon\n",
    "                ds_global = ds_global.transpose('time', 'lat', 'lon')\n",
    "                \n",
    "                mask_sum ={}\n",
    "                # replace the varibales with 0.00\n",
    "                for v in ds_global.data_vars:\n",
    "                    ds_global [v][:] = 0.00\n",
    "                    mask_sum[v] = np.zeros([1800, 3600])\n",
    "                \n",
    "                # loop over the ensembles\n",
    "                for ensemble in ensembles:\n",
    "                    \n",
    "                    #  read the files\n",
    "                    file_names = root_save+'probabilistic_daily/'+variable+'/*/'+\\\n",
    "                                           year+month+'/*'+ensemble+'.nc'\n",
    "                    file_names = glob.glob(file_names)\n",
    "                    \n",
    "                    #\n",
    "                    for file_name in file_names:\n",
    "                        print(file_name)\n",
    "                        ds = xr.open_dataset(file_name)\n",
    "                        #print(ds.data_vars)\n",
    "                        ds = ds.transpose('time', 'lat', 'lon') # making sure the order\n",
    "                        \n",
    "                        if ds.lon.values.max() > 180:\n",
    "                            # Asia\n",
    "                            ds1 = ds.sel(lon=slice(None, 179.975))\n",
    "                            ds2 = ds.sel(lon=slice(179.975, None))\n",
    "                            ds2['lon'] = ds2['lon'] - 360\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                        elif ds.lon.values.min() < -180:\n",
    "                            # North America\n",
    "                            ds1 = ds.sel(lon=slice(None, -179.975))\n",
    "                            ds2 = ds.sel(lon=slice(-179.975, None))\n",
    "                            ds1['lon'] = ds1['lon'] + 360\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                        else:\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds , ds_global, mask_sum)\n",
    "                    \n",
    "                    \n",
    "                    file_name = 'EM_Earth_probabilistic_daily_'+variable+'_global_'+\\\n",
    "                                year+month+'_'+ensemble+'.nc'\n",
    "                    folder_name = root_save+'probabilistic_daily_global/'+variable+'/'+\\\n",
    "                                year+month+'/'\n",
    "                    if not os.path.isdir(folder_name):\n",
    "                        os.makedirs(folder_name)\n",
    "                    \n",
    "                    for name, value in ds_global.items():\n",
    "                        for t in range(value.shape[0]):\n",
    "                            temp = np.array(value[t,:,:]) / mask_sum[name]\n",
    "                            temp[mask_sum[name] == 0] = -9999\n",
    "                            value[t, :, :] = temp\n",
    "                        ds_global[name] = value\n",
    "                    \n",
    "                    print(file_name)\n",
    "                    \n",
    "                    os.system('rm '+folder_name+file_name)\n",
    "                    \n",
    "                    encoding = {}\n",
    "                    for var in ds_global.data_vars:\n",
    "                        encoding[var] = {'_FillValue': -9999., 'zlib': True, 'complevel': 4}\n",
    "                        \n",
    "                    ds_global.to_netcdf(folder_name+file_name, unlimited_dims='time', encoding=encoding)\n",
    "                    \n",
    "\n",
    "def merging_EM_ERATH_deterministic  (variables = ['prcp','tdew','tmean'],\n",
    "                                     regions = ['Asia','Europe','Africa','NorthAmerica','Oceania','SouthAmerica'],\n",
    "                                     years = ['1950','1951','1952','1953','1954',\\\n",
    "                                              '1955','1956','1957','1958','1959',\\\n",
    "                                              '1960','1961','1962','1963','1964',\\\n",
    "                                              '1965','1966','1967','1968','1969',\\\n",
    "                                              '1970','1971','1972','1973','1974',\\\n",
    "                                              '1975','1976','1977','1978','1979',\\\n",
    "                                              '1980','1981','1982','1983','1984',\\\n",
    "                                              '1985','1986','1987','1988','1989',\\\n",
    "                                              '1990','1991','1992','1993','1994',\\\n",
    "                                              '1995','1996','1997','1998','1999',\\\n",
    "                                              '2000','2001','2002','2003','2004',\\\n",
    "                                              '2005','2006','2007','2008','2009',\\\n",
    "                                              '2010','2011','2012','2013','2014',\\\n",
    "                                              '2015','2016','2017','2018','2019'],\n",
    "                                     months = ['01','02','03','04','05','06',\\\n",
    "                                               '07','08','09','10','11','12'],\n",
    "                                     root_save = '../EM_Earth_v1/'):\n",
    "    \n",
    "    \n",
    "    for year in years:\n",
    "\n",
    "        for month in months:\n",
    "\n",
    "            for variable in variables:\n",
    "                \n",
    "                # file_name\n",
    "                file_name = root_save+'deterministic_raw_daily/'+variable+'/'+\\\n",
    "                'EM_Earth_deterministic_daily_'+variable+'_'+year+month+'.nc'\n",
    "                \n",
    "                # open dataset\n",
    "                ds_global = xr.open_dataset(file_name)\n",
    "                \n",
    "                # go from daily to hourly values\n",
    "                ds_temp = ds_global.copy()\n",
    "                temp = ds_temp['time'].copy()\n",
    "                ds_temp = ds_temp.drop(labels='time')\n",
    "                ds_temp['time'] = temp + np.timedelta64(23, 'h')\n",
    "                ds_temp = ds_temp.isel(time=[-2,-1])\n",
    "                ds_global = xr.merge([ds_global, ds_temp])\n",
    "                ds_global = ds_global.resample(time='1H').pad()\n",
    "                print(ds_global)\n",
    "                \n",
    "                # reorganize the variables to time, lat, lon\n",
    "                ds_global = ds_global.transpose('time', 'lat', 'lon')\n",
    "                \n",
    "                mask_sum ={}\n",
    "                # replace the varibales with 0.00\n",
    "                for v in ds_global.data_vars:\n",
    "                    ds_global [v][:] = 0.00\n",
    "                    mask_sum[v] = np.zeros([1800, 3600])\n",
    "                \n",
    "                #  read the files\n",
    "                file_names = root_save+'deterministic_hourly/'+variable+'/*/*'+\\\n",
    "                                       year+month+'*.nc'\n",
    "                file_names = glob.glob(file_names)\n",
    "                print(file_names)\n",
    "                \n",
    "                # loop over days\n",
    "                number_of_days = len(ds_global['time'])/24\n",
    "                \n",
    "                    #\n",
    "                    for file_name in file_names:\n",
    "                        print(file_name)\n",
    "                        ds = xr.open_dataset(file_name)\n",
    "                        #print(ds.data_vars)\n",
    "                        ds = ds.transpose('time', 'lat', 'lon') # making sure the order\n",
    "\n",
    "                        if ds.lon.values.max() > 180:\n",
    "                            # Asia\n",
    "                            ds1 = ds.sel(lon=slice(None, 179.975))\n",
    "                            ds2 = ds.sel(lon=slice(179.975, None))\n",
    "                            ds2['lon'] = ds2['lon'] - 360\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                        elif ds.lon.values.min() < -180:\n",
    "                            # North America\n",
    "                            ds1 = ds.sel(lon=slice(None, -179.975))\n",
    "                            ds2 = ds.sel(lon=slice(-179.975, None))\n",
    "                            ds1['lon'] = ds1['lon'] + 360\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds1, ds_global, mask_sum)\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds2, ds_global, mask_sum)\n",
    "                        else:\n",
    "                            ds_global, mask_sum = fill_cont_to_globe(ds , ds_global, mask_sum)\n",
    "\n",
    "\n",
    "                    file_name = 'EM_Earth_deterministic_hourly_'+variable+'_global_'+\\\n",
    "                                year+month+'.nc'\n",
    "                    folder_name = root_save+'deterministic_raw_daily_global/'+variable+'/'+\\\n",
    "                                year+month+'/'\n",
    "                    if not os.path.isdir(folder_name):\n",
    "                        os.makedirs(folder_name)\n",
    "\n",
    "                    for name, value in ds_global.items():\n",
    "                        for t in range(value.shape[0]):\n",
    "                            temp = np.array(value[t,:,:]) / mask_sum[name]\n",
    "                            temp[mask_sum[name] == 0] = -9999\n",
    "                            value[t, :, :] = temp\n",
    "                        ds_global[name] = value\n",
    "\n",
    "                    print(file_name)\n",
    "\n",
    "                    os.system('rm '+folder_name+file_name)\n",
    "\n",
    "                    encoding = {}\n",
    "                    for var in ds_global.data_vars:\n",
    "                        encoding[var] = {'_FillValue': -9999., 'zlib': True, 'complevel': 4}\n",
    "\n",
    "                    ds_global.to_netcdf(folder_name+file_name, unlimited_dims='time', encoding=encoding)\n",
    "\n",
    "\n",
    "def fill_cont_to_globe(ds, ds_global, mask_sum):\n",
    "    \n",
    "    row_start = np.where(np.array(ds_global.lat[:]) == ds.lat.max().item())\n",
    "    row_end   = np.where(np.array(ds_global.lat[:]) == ds.lat.min().item())\n",
    "    col_start = np.where(np.array(ds_global.lon[:]) == ds.lon.min().item())\n",
    "    col_end   = np.where(np.array(ds_global.lon[:]) == ds.lon.max().item())\n",
    "    row_start = row_start[0].item()\n",
    "    row_end   = row_end[0].item()\n",
    "    col_start = col_start[0].item()\n",
    "    col_end   = col_end[0].item()\n",
    "    \n",
    "    print(row_start, row_end, col_start, col_end)\n",
    "    \n",
    "    for v in ds.data_vars:\n",
    "        \n",
    "        #\n",
    "        datav = ds[v].values.copy()\n",
    "        datav[np.isnan(datav)] = 0 # replace NaN with 0\n",
    "        ds_global[v][:, row_start:row_end+1, col_start:col_end+1] = \\\n",
    "        ds_global[v][:, row_start:row_end+1, col_start:col_end+1] + datav\n",
    "\n",
    "        #\n",
    "        datav = ds[v].values.copy()\n",
    "        datav = np.array(datav[0,:,:])\n",
    "\n",
    "        datav[~np.isnan(datav)] = 1\n",
    "        datav[np.isnan(datav)] = 0\n",
    "        mask_sum[v][row_start:row_end+1, col_start:col_end+1] = \\\n",
    "        mask_sum[v][row_start:row_end+1, col_start:col_end+1] + datav\n",
    "        \n",
    "    \n",
    "    return ds_global, mask_sum \n",
    "\n",
    "def test_merging_EM_ERATH_ensmebles(variables = ['prcp','tdew','tmean','trange'],\n",
    "                                    regions = ['Asia','Europe','Africa','NorthAmerica','Oceania','SouthAmerica'],\n",
    "                                    years = ['1950','1951','1952','1953','1954',\\\n",
    "                                             '1955','1956','1957','1958','1959',\\\n",
    "                                             '1960','1961','1962','1963','1964',\\\n",
    "                                             '1965','1966','1967','1968','1969',\\\n",
    "                                             '1970','1971','1972','1973','1974',\\\n",
    "                                             '1975','1976','1977','1978','1979',\\\n",
    "                                             '1980','1981','1982','1983','1984',\\\n",
    "                                             '1985','1986','1987','1988','1989',\\\n",
    "                                             '1990','1991','1992','1993','1994',\\\n",
    "                                             '1995','1996','1997','1998','1999',\\\n",
    "                                             '2000','2001','2002','2003','2004',\\\n",
    "                                             '2005','2006','2007','2008','2009',\\\n",
    "                                             '2010','2011','2012','2013','2014',\\\n",
    "                                             '2015','2016','2017','2018','2019'],\n",
    "                                    months = ['01','02','03','04','05','06',\\\n",
    "                                              '07','08','09','10','11','12'],\n",
    "                                    ensembles = ['001','002','003','004','005',\\\n",
    "                                                 '006','007','008','009','010',\\\n",
    "                                                 '011','012','013','014','015',\\\n",
    "                                                 '016','017','018','019','020',\\\n",
    "                                                 '021','022','023','024','025'],\n",
    "                                    root_save = '../EM_Earth_v1/'):\n",
    "    \n",
    "    \n",
    "    A = 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c09ec787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the files from HTTP\n",
    "# download_EM_ERATH(years = ['1950','2019'],\n",
    "#                   months = ['01','02'],\n",
    "#                   ensembles = ['001','002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67639d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (time: 744, lat: 1800, lon: 3600)\n",
      "Coordinates:\n",
      "  * time            (time) datetime64[ns] 1950-01-01 ... 1950-01-31T23:00:00\n",
      "  * lat             (lat) float32 89.95 89.85 89.75 ... -89.75 -89.85 -89.95\n",
      "  * lon             (lon) float32 -179.9 -179.9 -179.8 ... 179.8 179.9 179.9\n",
      "Data variables:\n",
      "    prcp            (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n",
      "    prcp_corrected  (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n",
      "Attributes:\n",
      "    Dataset:    EM-Earth: Ensemble Meteorological Dataset for Planet Earth\n",
      "    Developer:  Guoqiang Tang et al. in Center for Hydrology, Coldwater Lab, ...\n",
      "    Type:       Deterministic station-reanalysis merged estimates\n",
      "['../EM_Earth_v1/deterministic_hourly/prcp/Oceania/EM_Earth_deterministic_hourly_Oceania_195001.nc', '../EM_Earth_v1/deterministic_hourly/prcp/NorthAmerica/EM_Earth_deterministic_hourly_NorthAmerica_195001.nc', '../EM_Earth_v1/deterministic_hourly/prcp/Asia/EM_Earth_deterministic_hourly_Asia_195001.nc', '../EM_Earth_v1/deterministic_hourly/prcp/Europe/EM_Earth_deterministic_hourly_Europe_195001.nc', '../EM_Earth_v1/deterministic_hourly/prcp/SouthAmerica/EM_Earth_deterministic_hourly_SouthAmerica_195001.nc', '../EM_Earth_v1/deterministic_hourly/prcp/Africa/EM_Earth_deterministic_hourly_Africa_195001.nc']\n",
      "../EM_Earth_v1/deterministic_hourly/prcp/Oceania/EM_Earth_deterministic_hourly_Oceania_195001.nc\n",
      "865 1448 2928 3599\n",
      "../EM_Earth_v1/deterministic_hourly/prcp/NorthAmerica/EM_Earth_deterministic_hourly_NorthAmerica_195001.nc\n",
      "62 830 3523 3599\n",
      "62 830 0 1686\n",
      "../EM_Earth_v1/deterministic_hourly/prcp/Asia/EM_Earth_deterministic_hourly_Asia_195001.nc\n",
      "86 1041 2048 3599\n"
     ]
    }
   ],
   "source": [
    "# merge the files\n",
    "# merging_EM_ERATH(years = ['1950','2019'],\n",
    "#                  months = ['01','02'],\n",
    "#                  ensembles = ['001','002'])\n",
    "\n",
    "\n",
    "merging_EM_ERATH_deterministic(years = ['1950','2019'],\n",
    "                               months = ['01','02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# ds_global = xr.open_dataset('../EM_Earth_v1/probabilistic_daily_global/prcp/200001/EM_Earth_probabilistic_daily_prcp_global_200001_001.nc')\n",
    "# ds_global = ds_global.sel(time=slice('2000-01-01'))\n",
    "# ds_global = ds_global.squeeze()\n",
    "# ds_global = ds_global['prcp'].to_dataframe()\n",
    "# ds_global = ds_global.reset_index()\n",
    "# ds_global = ds_global.drop(columns=['time'])\n",
    "# ds_global['lat'] = ds_global['lat'].round(2)\n",
    "# ds_global['lon'] = ds_global['lon'].round(2)\n",
    "# print(ds_global)\n",
    "\n",
    "\n",
    "# ds = xr.open_dataset('../EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/200001/EM_Earth_probabilistic_daily_prcp_NorthAmerica_200001_001.nc')\n",
    "# ds = ds.sel(time=slice('2000-01-01'))\n",
    "# ds = ds.squeeze()\n",
    "# ds = ds['prcp'].to_dataframe()\n",
    "# ds = ds.reset_index()\n",
    "# ds = ds.drop(columns=['time'])\n",
    "# ds['lat'] = ds['lat'].round(2)\n",
    "# ds['lon'] = ds['lon'].round(2)\n",
    "# print(ds)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# ds_new = pd.merge(ds_global, ds,  how='right', left_on=['lat','lon'], right_on = ['lat','lon'])\n",
    "# ds_new['diff'] = ds_new['prcp_x']-ds_new['prcp_y']\n",
    "# print(ds_new['diff'].abs().sum())\n",
    "# print(ds_new)\n",
    "\n",
    "\n",
    "# ds_new['prcp_x'].plot()\n",
    "# ds_new['prcp_y'].plot()\n",
    "# ds_new['diff'].plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8bb050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'strftime' (time: 31)>\n",
      "array(['1950-01-01', '1950-01-02', '1950-01-03', '1950-01-04',\n",
      "       '1950-01-05', '1950-01-06', '1950-01-07', '1950-01-08',\n",
      "       '1950-01-09', '1950-01-10', '1950-01-11', '1950-01-12',\n",
      "       '1950-01-13', '1950-01-14', '1950-01-15', '1950-01-16',\n",
      "       '1950-01-17', '1950-01-18', '1950-01-19', '1950-01-20',\n",
      "       '1950-01-21', '1950-01-22', '1950-01-23', '1950-01-24',\n",
      "       '1950-01-25', '1950-01-26', '1950-01-27', '1950-01-28',\n",
      "       '1950-01-29', '1950-01-30', '1950-01-31'], dtype=object)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1950-01-01 1950-01-02 ... 1950-01-31\n",
      "<xarray.Dataset>\n",
      "Dimensions:         (time: 744, lat: 1800, lon: 3600)\n",
      "Coordinates:\n",
      "  * time            (time) datetime64[ns] 1950-01-01 ... 1950-01-31T23:00:00\n",
      "  * lat             (lat) float32 89.95 89.85 89.75 ... -89.75 -89.85 -89.95\n",
      "  * lon             (lon) float32 -179.9 -179.9 -179.8 ... 179.8 179.9 179.9\n",
      "Data variables:\n",
      "    prcp            (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n",
      "    prcp_corrected  (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n",
      "Attributes:\n",
      "    Dataset:    EM-Earth: Ensemble Meteorological Dataset for Planet Earth\n",
      "    Developer:  Guoqiang Tang et al. in Center for Hydrology, Coldwater Lab, ...\n",
      "    Type:       Deterministic station-reanalysis merged estimates\n",
      "<xarray.DataArray 'strftime' ()>\n",
      "array('1950-01-01', dtype=object)\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1950-01-01\n",
      "<xarray.Dataset>\n",
      "Dimensions:         (time: 24, lat: 1800, lon: 3600)\n",
      "Coordinates:\n",
      "  * time            (time) datetime64[ns] 1950-01-01 ... 1950-01-01T23:00:00\n",
      "  * lat             (lat) float32 89.95 89.85 89.75 ... -89.75 -89.85 -89.95\n",
      "  * lon             (lon) float32 -179.9 -179.9 -179.8 ... 179.8 179.9 179.9\n",
      "Data variables:\n",
      "    prcp            (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n",
      "    prcp_corrected  (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n",
      "Attributes:\n",
      "    Dataset:    EM-Earth: Ensemble Meteorological Dataset for Planet Earth\n",
      "    Developer:  Guoqiang Tang et al. in Center for Hydrology, Coldwater Lab, ...\n",
      "    Type:       Deterministic station-reanalysis merged estimates\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yh/b1qy7zb96k980mcb2ps9n6d9t1c6zr/T/ipykernel_76367/2965841658.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'll' is not defined"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "ds = xr.open_dataset('../EM_Earth_v1/deterministic_raw_daily/prcp/EM_Earth_deterministic_daily_prcp_195001.nc')\n",
    "dates = ds['time'].dt.strftime('%Y-%m-%d')  #astype(str)\n",
    "print(dates)\n",
    "\n",
    "\n",
    "# go from daily to hourly values\n",
    "ds_temp = ds.copy()\n",
    "temp = ds_temp['time'].copy()\n",
    "ds_temp = ds_temp.drop(labels='time')\n",
    "ds_temp['time'] = temp + np.timedelta64(23, 'h')\n",
    "ds_temp = ds_temp.isel(time=[-2,-1])\n",
    "ds = xr.merge([ds, ds_temp])\n",
    "ds = ds.resample(time='1H').pad()\n",
    "print(ds)\n",
    "\n",
    "for i in np.arange(len(dates)):\n",
    "    print(dates[i])\n",
    "    mm = ds.sel(time=slice(dates[i], dates[i]))\n",
    "    print(mm)\n",
    "    ll\n",
    "\n",
    "ll\n",
    "\n",
    "temp.dt.year\n",
    "# pd.to_datetime(temp)\n",
    "print(temp)\n",
    "\n",
    "\n",
    "temp.astype(object).year\n",
    "\n",
    "ll\n",
    "\n",
    "for ff in temp:\n",
    "    print(ff)\n",
    "    print(ff)\n",
    "    ds_temp = ds.sel(time=ff)\n",
    "    print(ds_temp)\n",
    "    ll\n",
    "    \n",
    "\n",
    "\n",
    "ll\n",
    "\n",
    "\n",
    "ds_temp = ds.copy()\n",
    "\n",
    "temp = ds_temp['time'].copy()\n",
    "ds_temp = ds_temp.drop(labels='time')\n",
    "ds_temp['time'] = temp + np.timedelta64(23, 'h')\n",
    "#ds_temp['time'] = ds_temp['temp'] \n",
    "#ds_temp = ds_temp.drop(labels='temp')\n",
    "#ds_temp = ds_temp.resample(time='1H').pad()\n",
    "print(ds)\n",
    "print(ds['time'][0])\n",
    "print(ds['time'][-1])\n",
    "print(ds_temp['time'][-1])\n",
    "# ll\n",
    "#ds_temp = ds_temp.sel(time=\"1950-01-01T23:00:00\")\n",
    "ds_temp = ds_temp.isel(time=[-2,-1])\n",
    "print(ds_temp)\n",
    "\n",
    "print(datetime.now())\n",
    "ds = xr.merge([ds, ds_temp])\n",
    "print(datetime.now())\n",
    "print(ds)\n",
    "ds = ds.resample(time='1H').pad()\n",
    "print(ds)\n",
    "print(ds['time'][0])\n",
    "print(ds['time'][-1])\n",
    "ll\n",
    "\n",
    "#print(ds['time'][0].astype(datetime))\n",
    "A = datetime(ds['time'][0])\n",
    "print(A)\n",
    "ll\n",
    "\n",
    "time_index2 = pd.date_range(start=str(ds['time'][0]),end=str(ds['time'][-1]), freq=\"D\")\n",
    "print(time_index2)\n",
    "ll\n",
    "#ds['time'][:] = ds['time'][:]+np.timedelta64(23, 'h')\n",
    "ds['lat'][-1] =1000 \n",
    "ds = ds.resample(time='1H').pad() #pad() #pad()\n",
    "print(ds)\n",
    "print(ds['time'][0])\n",
    "print(ds['time'][-1])\n",
    "ll\n",
    "\n",
    "ds1 = xr.open_dataset('../EM_Earth_v1/deterministic_hourly/prcp/Asia/EM_Earth_deterministic_hourly_Asia_195001.nc')\n",
    "print(ds1['time'][0])\n",
    "\n",
    "#ds['time']=ds1['time']\n",
    "#print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7412b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'date' ()>\n",
      "array('1950-01-01', dtype='<U10')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1950-01-01\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument 'arg' has incorrect type (expected str, got numpy.str_)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yh/b1qy7zb96k980mcb2ps9n6d9t1c6zr/T/ipykernel_76367/964289752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36msel\u001b[0;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \"\"\"\n\u001b[1;32m   2473\u001b[0m         \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meither_dict_or_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2474\u001b[0;31m         pos_indexers, new_indexes = remap_label_indexers(\n\u001b[0m\u001b[1;32m   2475\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/xarray/core/coordinates.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     }\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     pos_indexers, new_indexes = indexing.remap_label_indexers(\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(data_obj, indexers, method, tolerance)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped_indexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0midxr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mpos_indexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/xarray/core/indexes.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, labels, method, tolerance)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_query_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/xarray/core/indexes.py\u001b[0m in \u001b[0;36m_query_slice\u001b[0;34m(index, label, coord_name, method, tolerance)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;34m\"cannot use ``method`` argument if any indexers are slice objects\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         )\n\u001b[0;32m---> 91\u001b[0;31m     indexer = index.slice_indexer(\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0m_sanitize_slice_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0m_sanitize_slice_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mslice_indexer\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# For historical reasons DatetimeIndex by default supports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_indexer\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   5275\u001b[0m         \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m         \"\"\"\n\u001b[0;32m-> 5277\u001b[0;31m         \u001b[0mstart_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5279\u001b[0m         \u001b[0;31m# return a slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_locs\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   5474\u001b[0m         \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5476\u001b[0;31m             \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_slice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5478\u001b[0m             \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m   5384\u001b[0m         \u001b[0;31m# For datetime indices label may be a string that has to be converted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5385\u001b[0m         \u001b[0;31m# to datetime boundary according to its resolution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5386\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5388\u001b[0m         \u001b[0;31m# we need to look up the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/myenv38/lib/python3.8/site-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cast_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"freqstr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inferred_freq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                 \u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_time_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDateParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"slice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'arg' has incorrect type (expected str, got numpy.str_)"
     ]
    }
   ],
   "source": [
    "for i in np.arange(len(dates)):\n",
    "    print(dates[i].astype(str))\n",
    "    mm = ds.sel(time=slice(dates[i].astype(str), dates[i].astype(str)))\n",
    "    print(mm)\n",
    "    ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b929642c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.timedelta64(23,'h')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.timedelta64(23, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fab8af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (time: 721, lat: 1800, lon: 3600)\n",
      "Coordinates:\n",
      "  * time            (time) datetime64[ns] 1950-01-01 ... 1950-01-31\n",
      "  * lat             (lat) float32 89.95 89.85 89.75 ... -89.75 -89.85 -89.95\n",
      "  * lon             (lon) float32 -179.9 -179.9 -179.8 ... 179.8 179.9 179.9\n",
      "Data variables:\n",
      "    prcp            (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n",
      "    prcp_corrected  (time, lat, lon) float32 nan nan nan nan ... nan nan nan nan\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af923c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' ()>\n",
      "array('1950-01-31T00:00:00.000000000', dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1950-01-31\n"
     ]
    }
   ],
   "source": [
    "print(ds['time'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bff58588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' ()>\n",
      "array('1950-01-01T00:00:00.000000000', dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1950-01-01\n",
      "<xarray.DataArray 'time' ()>\n",
      "array('1950-01-31T00:00:00.000000000', dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1950-01-31\n",
      "<xarray.DataArray 'time' ()>\n",
      "array('1950-01-01T00:00:00.000000000', dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1950-01-01\n",
      "<xarray.DataArray 'time' ()>\n",
      "array('1950-01-31T00:00:00.000000000', dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1950-01-31\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "ds = xr.open_dataset('../EM_Earth_v1/deterministic_raw_daily/prcp/EM_Earth_deterministic_daily_prcp_195001.nc')\n",
    "print(ds['time'][0])\n",
    "print(ds['time'][-1])\n",
    "\n",
    "ds = ds.resample(time='1H').pad() #pad() #pad()\n",
    "print(ds['time'][0])\n",
    "print(ds['time'][-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f82ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hello!\n",
    "\n",
    "I am trying to move from a dayily file to a hourly file, however my try with xarray was not sucessful to give me what I want,\n",
    "for example, if I use the resample and pad then the model simulation is used in the area are in the model\n",
    "with the model simulation in which is located in the area in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a105fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the input file name: ../EM_Earth_v1/deterministic_hourly/prcp/Africa/EM_Earth_deterministic_hourly_Africa_195001.nc\n",
      "afea8267e9d46598e463e3da1eab7ec26fee77297962d144496653c0166b6086\n"
     ]
    }
   ],
   "source": [
    "# Python program to find SHA256 hash string of a file\n",
    "import hashlib\n",
    " \n",
    "filename = input(\"Enter the input file name: \")\n",
    "sha256_hash = hashlib.sha256()\n",
    "with open(filename,\"rb\") as f:\n",
    "    # Read and update hash string value in blocks of 4K\n",
    "    for byte_block in iter(lambda: f.read(4096),b\"\"):\n",
    "        sha256_hash.update(byte_block)\n",
    "    print(sha256_hash.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5149de",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/shg096/Downloads/README.txt\n",
    "xr.open('../EM_Earth_v1/deterministic_hourly/prcp/Africa/EM_Earth_deterministic_hourly_Africa_195001.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f03f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                File_Name  \\\n",
      "0        /globusdata/6/published/publication_542/submi...   \n",
      "1        /globusdata/6/published/publication_542/submi...   \n",
      "2        /globusdata/6/published/publication_542/submi...   \n",
      "3        /globusdata/6/published/publication_542/submi...   \n",
      "4        /globusdata/6/published/publication_542/submi...   \n",
      "...                                                   ...   \n",
      "562800   /globusdata/6/published/publication_542/submi...   \n",
      "562801   /globusdata/6/published/publication_542/submi...   \n",
      "562802   /globusdata/6/published/publication_542/submi...   \n",
      "562803   /globusdata/6/published/publication_542/submi...   \n",
      "562804   /globusdata/6/published/publication_542/submi...   \n",
      "\n",
      "                                                   sha256  \\\n",
      "0        4f5ef9a7d5edcf894bdd7f7c5330b0e8107ee4350cad2...   \n",
      "1        e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b9...   \n",
      "2        e749d15780b4aba883c06ed2a38418a04b21bae56f2c2...   \n",
      "3        cf66662969834e9f8013351aa4855fdb2ea582828a343...   \n",
      "4        8b2f626ade8b1b727f4d78d8a883abde1a6058fc03d30...   \n",
      "...                                                   ...   \n",
      "562800   132831d6f1d66db54652766741002132ce2e68bb779c4...   \n",
      "562801   bd4a273e2bb5575b655527972c70d2880f138b0628d17...   \n",
      "562802   38e945b3dfef3f0fd4c1d8b420df260393ef517a168af...   \n",
      "562803   f43b8f812b3347b0c772dd38b746df631d6fae452275e...   \n",
      "562804   47dc5729a0c4f869f5fd3b777fa732cd5d92633876eb3...   \n",
      "\n",
      "                            File_Type  \n",
      "0                   Plain Text File    \n",
      "1                          Markdown    \n",
      "2                   Plain Text File    \n",
      "3                   Plain Text File    \n",
      "4        Microsoft Word for Windows    \n",
      "...                               ...  \n",
      "562800                         HDF5    \n",
      "562801                         HDF5    \n",
      "562802                         HDF5    \n",
      "562803                         HDF5    \n",
      "562804                         HDF5    \n",
      "\n",
      "[562805 rows x 3 columns]\n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/trange/NorthAmerica/199605/EM_Earth_probabilistic_daily_trange_NorthAmerica_199605_006.nc \n",
      "                                                File_Name  \\\n",
      "410649   /globusdata/6/published/publication_542/submi...   \n",
      "411747   /globusdata/6/published/publication_542/submi...   \n",
      "416453   /globusdata/6/published/publication_542/submi...   \n",
      "417342   /globusdata/6/published/publication_542/submi...   \n",
      "417398   /globusdata/6/published/publication_542/submi...   \n",
      "421231   /globusdata/6/published/publication_542/submi...   \n",
      "424603   /globusdata/6/published/publication_542/submi...   \n",
      "425435   /globusdata/6/published/publication_542/submi...   \n",
      "425623   /globusdata/6/published/publication_542/submi...   \n",
      "426345   /globusdata/6/published/publication_542/submi...   \n",
      "426396   /globusdata/6/published/publication_542/submi...   \n",
      "428988   /globusdata/6/published/publication_542/submi...   \n",
      "\n",
      "                                                   sha256 File_Type  \n",
      "410649   5d7acf2438cc7ae61ed85f102a4b5cf31e664ee425a00...    HDF5    \n",
      "411747   7263002c842e81d3035b6c03dc96923b5639cf1e0d8ee...    HDF5    \n",
      "416453   7864254b9df09ca4234935124f842bad84d19af8ab3d7...    HDF5    \n",
      "417342   480d0f8dce1f5bf4cec59c0cd36b72f70d89e6acb57e5...    HDF5    \n",
      "417398   6ec99dbbd4c4a1b1f9211591d9352e75783f9b1241e9b...    HDF5    \n",
      "421231   139f6efef76723072618823cae72917efa30521256872...    HDF5    \n",
      "424603   ffdcffb3220b92a8f756b75285da9b4ab31c72084163d...    HDF5    \n",
      "425435   346cbf89d0d9727847ed173cfcdb2d51cc4b4a1c6c334...    HDF5    \n",
      "425623   b1c281e18551db4bc7c742c96299b50430478916cdb7f...    HDF5    \n",
      "426345   1299f9190c85f795bb5bc16bd72ade29fa0e867695958...    HDF5    \n",
      "426396   8e502cfa5b51bb91f2061e8c8095a849c7e95b4f698a7...    HDF5    \n",
      "428988   b590da1ac7fceb2803bc47ccc26d41edb1e2bd1b5f2e0...    HDF5    \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195112/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195112_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195103/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195103_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195110/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195110_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195101/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195101_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195109/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195109_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195106/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195106_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195104/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195104_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195102/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195102_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195107/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195107_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195111/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195111_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195105/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195105_006.nc \n",
      " /globusdata/6/published/publication_542/submitted_data/EM_Earth_v1/probabilistic_daily/prcp/NorthAmerica/195108/EM_Earth_probabilistic_daily_prcp_NorthAmerica_195108_006.nc \n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/shg096/Downloads/frdr-checksums-0547.csv')\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(df.iloc[10]['File_Name'])\n",
    "\n",
    "\n",
    "\n",
    "terms = ['NorthAmerica', '006', '1951', 'prcp']\n",
    "\n",
    "df_slice = df.copy()\n",
    "\n",
    "for term in terms:\n",
    "\n",
    "    df_slice = df_slice[df_slice['File_Name'].str.contains(term)]\n",
    "\n",
    "\n",
    "# df_slice = df['File_Name'].filter(like='NorthAmerica')\n",
    "\n",
    "print(df_slice)\n",
    "\n",
    "for index, row in df_slice.iterrows():\n",
    "    \n",
    "    #\n",
    "    print(row['File_Name'])\n",
    "    \n",
    "    \n",
    "print(len(df_slice))\n",
    "\n",
    "\n",
    "# get the model simulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f982456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dfb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "myenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
